{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGyeFRYISsAcvtJWpz/1ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felix-Think/Capstone_SamSung/blob/master/Text_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step1. Install and load Datasets\n",
        "## We use MS_MACRO"
      ],
      "metadata": {
        "id": "IbSJSo_-O1rx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RsqlgIZMKAy",
        "outputId": "63f3a113-bf7e-4814-9a07-dd7a031ea8ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "#Load MSMACRO\n",
        "ds = load_dataset(\"microsoft/ms_marco\", \"v2.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Extract dataset"
      ],
      "metadata": {
        "id": "Gx2-aAxEPNTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "subset = ds['test']"
      ],
      "metadata": {
        "id": "A1lLsIszM0Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQMBvSBONTPk",
        "outputId": "6941f48a-38c9-41d9-997e-2b0b05879a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
              "    num_rows: 101092\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract text\n",
        "#We only use sample with type == entity\n",
        "#Open this link https://huggingface.co/datasets/microsoft/ms_marco and get features in dataset\n",
        "corpus = []\n",
        "for text in subset:\n",
        "    query_type = text['query_type']\n",
        "    if query_type != 'ENTITY':\n",
        "        continue\n",
        "    query_id = text['query_id']\n",
        "    query_sdtr = text['query']\n",
        "    passages_dict = text['passages']\n",
        "    is_selected_lst = passages_dict['is_selected']\n",
        "    passage_text_lst = passages_dict['passage_text']\n",
        "    corpus += passage_text_lst"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sGZaPkJlPr16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RCXQrA86KjI",
        "outputId": "fff26e3b-0981-420c-ce3b-375aedd5ed92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9PQOWSh-6KDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Presentation"
      ],
      "metadata": {
        "id": "SZJWyoIwWMq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "def create_dictionary(corpus):\n",
        "    dictionary = []\n",
        "    for doc in corpus:\n",
        "        normalized_doc = text_normalize(doc)\n",
        "        tokens = tokenize(normalized_doc)\n",
        "        for token in tokens:\n",
        "            if token not in dictionary:\n",
        "                dictionary.append(token)\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "oXzJwrQITxTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(text, dictionary):\n",
        "    count_word_lst = {work: 0 for work in dictionary}\n",
        "    normalized_text = text_normalize(text)\n",
        "    tokens = tokenize(normalized_text)\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            count_word_lst[token] += 1\n",
        "        except:\n",
        "            continue\n",
        "    vector = list(count_word_lst.values())\n",
        "    return vector"
      ],
      "metadata": {
        "id": "38LBhybyXNSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing"
      ],
      "metadata": {
        "id": "DoDTEKpsbbJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_doc_term_matrix(corpus, dictionary):\n",
        "    doc_term_matrix = {}\n",
        "    for idx, doc in enumerate(corpus[:10000]):\n",
        "        vector = vectorize(doc, dictionary)\n",
        "        doc_term_matrix[(doc, idx)] = vector\n",
        "    return doc_term_matrix"
      ],
      "metadata": {
        "id": "anHC2XJVbGq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Normalization"
      ],
      "metadata": {
        "id": "FIqz-1cgcYYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase\n",
        "def text_lowercase(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "JBB9JS8UcWC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation\n",
        "import string\n",
        "remove_charts = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    for char in remove_charts:\n",
        "        text = text.replace(char, ' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "QbNbhqgJcwZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopword Removal\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    tokens = tokenize(text)\n",
        "    non_stop_words = [token for token in tokens if token not in stopwords_list]\n",
        "    new_text = ' '.join(non_stop_words)\n",
        "    return new_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGgNgHi7dFKl",
        "outputId": "9d48df68-6d5e-49f4-c23c-bbc1bd27e9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steaming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "def stemming(text):\n",
        "    tokens = tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(token) for token in tokens]\n",
        "    new_text = ' '.join(stemmed_words)\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "roIz0bYvd5DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "def text_normalize(text):\n",
        "    text = text_lowercase(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = stemming(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NkNXe-L3e3dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ranking"
      ],
      "metadata": {
        "id": "kGxozPP8fHag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def cosine_similarity(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    dot_product = a.dot(b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)"
      ],
      "metadata": {
        "id": "Mo88YxRRfGKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ranking(query, dictionary, doc_term_matrix):\n",
        "    query_vector = vectorize(query, dictionary)\n",
        "    scores = []\n",
        "    for doc_infor, doc_vector in doc_term_matrix.items():\n",
        "        sim = cosine_similarity(query_vector, doc_vector)\n",
        "        scores.append((sim, doc_infor))\n",
        "    scores.sort(reverse=True)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "EBOktCoefiTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = create_dictionary(corpus)\n"
      ],
      "metadata": {
        "id": "JNu1iIvFiFVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_term_matrix = create_doc_term_matrix(corpus, dictionary)"
      ],
      "metadata": {
        "id": "qlSkV7jU3PmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fQ3RbaYR3PUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ranking code and Result"
      ],
      "metadata": {
        "id": "Lvv21Iz7iF5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_list = ['What is the official languages in Fiji']\n",
        "top_k = 10\n",
        "\n",
        "for query in query_list:\n",
        "    scores = ranking(query, dictionary, doc_term_matrix)\n",
        "    print(f'Query: {query}')\n",
        "    print('==Relevent docs==')\n",
        "    for idx, (doc_score, doc_content) in enumerate(scores[:top_k]):\n",
        "        print(f'Top {idx+1}: {doc_score}')\n",
        "        print(doc_content)\n",
        "        print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7aVFNkpiJlg",
        "outputId": "ad866d7d-33fb-400d-9632-0ebf384d154f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the official languages in Fiji\n",
            "==Relevent docs==\n",
            "Top 1: 0.5319951765989316\n",
            "('New Zealand Language Official Languages. While English is the predominant language spoken in New Zealand, there are two actual official languages in New Zealand. Maori became an official language in 1987 while in April 2006, New Zealand became the first country to declare sign language as an official language, alongside Maori. New Zealand Sign Language, or NZSL, is the main language of the deaf community in New Zealand. Maori is only used in New Zealand and nowhere else in the world. Despite its official status, the language continues to struggle against being lost.', 7925)\n",
            "\n",
            "\n",
            "Top 2: 0.4479546293064525\n",
            "('While English is the predominant language spoken in New Zealand, there are two actual official languages in New Zealand. Maori became an official language in 1987 while in April 2006, New Zealand became the first country to declare sign language as an official language, alongside Maori. New Zealand Sign Language, or NZSL, is the main language of the deaf community in New Zealand. Te Reo Maori - the Maori Language. Maori is only used in New Zealand and nowhere else in the world. Despite its ...', 7921)\n",
            "\n",
            "\n",
            "Top 3: 0.4389512813061471\n",
            "('Auslan is related to British Sign Language (BSL) and New Zealand Sign Language (NZSL); the three have descended from the same parent language, and together comprise the BANZSL language family. New Zealand Sign Language or NZSL - The main language of the Deaf community in New Zealand. It became an official language of New Zealand in April 2006, alongside Maori and English. Like other natural sign languages, it was devised by and for Deaf people, with no linguistic connection to a spoken or written language, and it is fully capable of expressing anything a fluent signer wants to say.', 7924)\n",
            "\n",
            "\n",
            "Top 4: 0.428845013935118\n",
            "('Chairman PAL, Fakhar Zaman said all the languages spoken in Pakistan are the national languages of Pakistan and Urdu besides being a national language is the official and Lingua of Franca of the country.', 3319)\n",
            "\n",
            "\n",
            "Top 5: 0.40574111245982464\n",
            "('Pakistan is composed of two main groups of languages: Indo-Iranian and Indo-Aryan. Indo-Aryan group of languages: Urdu. Punjabi. Sindhi. Sairaki. Dardic languages like Shina and Kashmiri. Indo-Iranian languages: Pashto. Balochi. Then there is one Dravidian language: Brahui. These above are the main languages of Pakistan. There are minor languages in Pakistan too: Languages of Pakistan - Wikipedia. India is composed of four main groups of languages: Indo-Aryan, Dravidian, Austro-Asiatic, Sino Tibetan, Tai-kadai and Great Andamese languages. Indo-Aryan language family:', 3322)\n",
            "\n",
            "\n",
            "Top 6: 0.40414518843273806\n",
            "('Languages: Pakistan is composed of two main groups of languages: Indo-Iranian and Indo-Aryan. Indo-Aryan group of languages: Urdu. Punjabi. Sindhi. Sairaki. Dardic languages like Shina and Kashmiri. Indo-Iranian languages: Pashto. Balochi. Then there is one Dravidian language: Brahui. These above are the main languages of Pakistan', 3321)\n",
            "\n",
            "\n",
            "Top 7: 0.36961063547728645\n",
            "('List of programming languages. The aim of this list of programming languages is to include all notable programming languages in existence, both those in current use and historical ones, in alphabetical order, except for dialects of BASIC, esoteric programming languages, and markup languages. A# .NET.', 4675)\n",
            "\n",
            "\n",
            "Top 8: 0.3689323936863109\n",
            "('Types of Sign Language: BANZSL, or British, Australian and New Zealand Sign Language - Is the language of which British Sign Language (BSL), Auslan and New Zealand Sign Language (NZSL) may be considered dialects. These three languages may technically be considered dialects of a single language (BANZSL) due to their use of the same grammar, manual alphabet, and the high degree of lexical sharing (overlap of signs).', 7922)\n",
            "\n",
            "\n",
            "Top 9: 0.3651483716701107\n",
            "('In addition, incomplete knowledge about many indigenous languages makes it difficult to determine the difference between a dialect and a language on the one hand, a family (composed of languages), and a stock (composed of families or very different languages) on the other.', 1212)\n",
            "\n",
            "\n",
            "Top 10: 0.3651483716701107\n",
            "('Emotive language is a very common language technique, not only because it is persuasive, but because it naturally occurs in everyday speech. Keep in mind that some word choices evoke an emotional response, but are not so much emotive language as loaded language.', 9980)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-j1DobPUjZ0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}